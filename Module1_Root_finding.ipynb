{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Module1_Root_finding.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gachet/000mis-colabs/blob/master/Module1_Root_finding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-gwfmOo9ra",
        "colab_type": "text"
      },
      "source": [
        "## Module 1: Root-finding methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6X1eC9ieo9re",
        "colab_type": "text"
      },
      "source": [
        "*Material for this worksheet can be found in Chapters 2 of the AE2220-I lecture notes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R6dlSKIo9rf",
        "colab_type": "text"
      },
      "source": [
        "### Problem statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJq9HB00o9rg",
        "colab_type": "text"
      },
      "source": [
        "A root-finding method is a numerical method or algorithm to find a value of $x\\in[a,b]$,such that $f(x) = 0$, \n",
        "where $f$ is a continuous real function of 1-variable on the interval $[a,b]$, written \n",
        "$$\n",
        "f:\\mathbb{R}\\rightarrow\\mathbb{R}, f\\in C^0([a,b]).\n",
        "$$\n",
        "\n",
        "A value of $x$ satisfying $f(x)=0$ is called a *root* of the function $f$.\n",
        "\n",
        "Except for special cases, it is usually not possible to find analytic expressions for the roots.  For example, consider expressions for the roots of polynomials: linear - trivial, quadratic - simple, cubic/quartic - possible, quintic and above - impossible (see Abelâ€“Ruffini theorem <a href=https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem>[1]</a>). \n",
        "\n",
        "Thus computational methods for root-finding problems must be iterative in nature:\n",
        "\n",
        "- Start with an initial approximation of the root $x_0$.\n",
        "- Construct a sequence of $x_k = \\phi(x_{k-1})$ iterates, for a suitable function $\\phi(x)$, with the expectation/hope that $\\lim_{k\\rightarrow \\infty} f(x_k) = 0$.\n",
        "\n",
        "Three important considerations for iterative methods are:\n",
        "\n",
        "1. Conditions for convergence (i.e. \"Is the root eventually found?\")\n",
        "2. Convergence *rate* (i.e. \"How many iterations until the root is found?\")\n",
        "3. Stopping criterion (i.e. \"When can we stop iterating?\")\n",
        "\n",
        "These will be examined in detail with the help of plots and sample codes in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5mUMd4o9ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwUyUHJEo9rn",
        "colab_type": "text"
      },
      "source": [
        "In root-finding - and whenever we attempt to solve a problem numerically - we should ask two questions:\n",
        "\n",
        "- Does a solution <b>exist</b>?\n",
        "- Is that solution <b>unique</b>?\n",
        "\n",
        "Generally speaking, we are most happy if the answer to both is \"yes\".  If the answer to the first is \"no\", our numerical methods should ideally fail - e.g. report an error, as the problem has no answer.  If the answer to the second is \"no\", then there are multiple solutions, yet all our numerical methods below will return exactly <b>one</b> solution.  Then we should be aware that our numerical method is giving only a partial answer.\n",
        "\n",
        "The code given below plots the graphs for the following functions:\n",
        "\n",
        "$f_\\text{poly}(x) = x^3 - x + 1$\n",
        "\n",
        "$f_\\text{tan}(x) = \\text{tan}(x) - x$\n",
        "\n",
        "$f_\\text{cos}(x) = \\text{cos}(x) - 2x$\n",
        "\n",
        "$f_\\text{exp}(x) = \\text{e}^x - 2x$\n",
        "\n",
        "Run the code for each function and try and visualize if the functions above have a unique solution in a particular interval.  Find intervals $[a,b]$ where (i) multiple solutions exist, (ii) no solutions exist.  Note: plotting $\\tan(x)$ and $x$ separately and examining where they cross can give more insight than just plotting $\\tan(x)-x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4jLfT3Do9ro",
        "colab_type": "code",
        "outputId": "295def39-2d7c-4ddd-d2fd-ac3d1743f8d9",
        "colab": {}
      },
      "source": [
        "a,b = -2,2\n",
        "\n",
        "def f_poly(x): return x**3 - x + 1. \n",
        "def dfdx_poly(x): return 3.*x**2 - 1.  ### Derivatives are needed later...\n",
        "def d2fdx2_poly(x): return 6.*x\n",
        "\n",
        "def f_tan(x): return np.tan(x)-x\n",
        "def dfdx_tan(x): return np.tan(x)**2 \n",
        "def d2fdx2_tan(x): return 2.*np.sin(x)/np.cos(x)**3\n",
        "\n",
        "def f_cos(x): return np.cos(x) - 2.* x \n",
        "def dfdx_cos(x): return -np.sin(x) - 2.\n",
        "def d2fdx2_cos(x): return -np.cos(x)\n",
        "\n",
        "def f_exp(x): return math.exp(x) - 2.*x\n",
        "def dfdx_exp(x): return math.exp(x) - 2.\n",
        "def d2fdx2_exp(x): return math.exp(x)\n",
        "\n",
        "xx = np.linspace(a,b,101)\n",
        "\n",
        "plt.plot(xx, f_poly(xx) , '-b')\n",
        "plt.axhline(y=0,xmin=-2,xmax=2,color='k')  # Plot the y=0 line\n",
        "\n",
        "#plt.ylim(-2,6) # for f_tan"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x2b1336ad5358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSoiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2b1336a63320>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O9dRsswo9rt",
        "colab_type": "text"
      },
      "source": [
        "###  Algorithm #1: Recursive bisection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRido5E7o9ru",
        "colab_type": "text"
      },
      "source": [
        "In the simplest of terms, recursive bisection is a root-finding method that repeatedly bisects an interval and then selects a subinterval in which a root must lie for further processing. It is a very simple and robust method, but also relatively slow. Let us implement the method according to the algorithm from the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo6DF32Qo9rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recursive_bisection(ff, a, b, min_error=1.e-8):\n",
        "    assert ff(a) * ff(b) < 0.                ### check the initial interval contains a root\n",
        "    ai, bi = float(a), float(b)              ### set initial interval\n",
        "    plotint = [(ai,bi)]\n",
        "    while 0.5*np.abs(bi-ai) > min_error:     ### check that we're not converged\n",
        "        ci = (ai + bi)/2                     ### midpoint of current interval\n",
        "        if ff(ai) * ff(ci) < 0:              ### select which 1/2 interval to continue with\n",
        "            bi = ci\n",
        "        else:\n",
        "            ai = ci\n",
        "        plotint.append((ai,bi))              ### store interval for plotting\n",
        "    return ci, plotint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh8tLzzzo9ry",
        "colab_type": "text"
      },
      "source": [
        "The following plotting function visualizes the progress of the algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJyddxofo9r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bisection_progress_plot(fb, a, b):\n",
        "    xx = np.linspace(a,b,101)\n",
        "    plt.figure(figsize=(14,10))\n",
        "    plt.plot(xx, fb(xx), '-b')               ### plot function\n",
        "    for i,(ai,bi) in enumerate(plotint):     ### plot intervals/midpoints\n",
        "        height = -4.*i/len(plotint)\n",
        "        plt.plot([ai,bi], height*np.array([1.,1.]), '--or')\n",
        "        plt.plot([(ai+bi)/2,(ai+bi)/2], height*np.array([1.,1.]), '-oy')\n",
        "        plt.text(ai-(b-a)/40., height, r\"$%d$\"%i, fontsize=10)\n",
        "    plt.ylim(-4,4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFf6PSl0o9r4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xreference, plotint = recursive_bisection(f_poly, a, b, 1.e-6)\n",
        "print('Approximation:',xreference)\n",
        "bisection_progress_plot(f_poly, a, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWfBmqy7o9r7",
        "colab_type": "text"
      },
      "source": [
        "The only modification we have made is to check initially if a root exists in the interval (try removing this and see what happens for an interval for which the condition is not satisfied).  Also the intermediate intervals are stored so that we can plot them later and see the progress of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngBuzkIro9r8",
        "colab_type": "text"
      },
      "source": [
        "#### Convergence plot - Measures of error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzlgZkXSo9r9",
        "colab_type": "text"
      },
      "source": [
        "We are interested in methods that <i>converge</i> quickly, i.e. for which the error reduces rapidly as $k$ increases.  We define the **true error** with respect to the exact solution $\\tilde x$:\n",
        "$$\n",
        "\\tilde e_k := x_k - \\tilde x.\n",
        "$$\n",
        "However $\\tilde x$ is unknown - it's the quantity we're trying to find!  We say $e_k$ is not *computable*.  As a compromise we plot the **approximate error** with respect to the solution on the <i>final iteration</i> - using this as a reference solution, under the assumption that it is more accurate than the solution at all other iterations, and therefore estimates of the error will not be too far off:\n",
        "$$\n",
        "\\tilde e_k \\simeq \\hat e_k := x_k - x_{k,\\text{final}}.\n",
        "$$\n",
        "This is not ideal, as $x_{k,\\text{final}}$ is only known at the end of the process, and if the process doesn't converge our reference solution is bad, and $\\hat e_k$ may be zero even though $f(x_k)=0$ is not satisfied.  Therefore the  **residual error** is useful:\n",
        "$$\n",
        "\\epsilon_k := f(x_k)\n",
        "$$\n",
        "which is easy to compute, and only zero if the equation is satisfied.\n",
        "\n",
        "Finally a particular algorithm may have an **error estimate**.  In recursive bisection we have a guarantee that the root is in the interval $[a_k, b_k]$, giving us an upper bound on the error of:\n",
        "$$\n",
        "\\bar e_k := \\frac{b_k - a_k}{2}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnZfS79o9r-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bisection_convergence(plotint,xrefer,f_bis):\n",
        "    n = len(plotint)-1\n",
        "    nn = range(n)\n",
        "    error_estimate, error_approx, error_residual = np.zeros(n), np.zeros(n), np.zeros(n)\n",
        "    for i in range(n):\n",
        "        ai, bi = plotint[i][0], plotint[i][1]\n",
        "        ci = (ai + bi)/2\n",
        "        error_estimate[i] = (bi - ai)/2\n",
        "        error_approx[i]     = np.abs(ci - xrefer)\n",
        "        error_residual[i] = np.abs(f_bis(ci))\n",
        "    \n",
        "    plt.plot(nn[:-1], np.log10(error_approx[:-1]), '-o', label=r'$\\hat e_k$ - Approximate error')\n",
        "    plt.plot(nn, np.log10(error_estimate), '-o', label=r'$\\bar e$ - RB Error estimate')\n",
        "    plt.plot(nn, np.log10(error_residual), '-o', label=r'$\\epsilon$ - Residual error')\n",
        "    plt.xlabel(r'$n$', fontsize=20)\n",
        "    plt.ylabel(r'$\\mathrm{log}_{10}(e)$', fontsize=20)\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcVPrqkoo9sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bisection_convergence(plotint,xreference,f_poly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUPZFWtdo9sF",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1:**\n",
        "\n",
        "**(a) For a given min_error $\\epsilon$, predict the minimum number of iterations $N$ required.**  Hint: If $c_1 = \\frac {a+b}{2}$ is the midpoint of the initial interval and $c_n$ is the midpoint of the interval in the nth step, then the difference between $c_n$ and a solution $c$ is bounded by $|c_n-c| \\leq \\frac{| b-a |}{2^n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWc2tvBro9sI",
        "colab_type": "text"
      },
      "source": [
        "**(b) Consider the function $f(x) =\\tan x - x$ in the interval $(-0.5,5).$**\n",
        "\n",
        "  0. **Plotting the function $y=\\tan x$ and $y = x$ earlier has given as an idea of the roots in the interval. Try finding the first non-zero positive root by starting with the interval $(1.0,5.0)$. Does the method converge to the first root? Why not? Is it possible to find the root by changing the interval?**\n",
        "\n",
        "  0. **Try to find the first root using the same interval.** Hint: Consider rewriting the function such that it is continous in the interval (1.0,5.0) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVTIXtqpo9sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO\n",
        "# You can call the already written functions recursive_bisection and bisection_progress_plot\n",
        "# directly by using arguments according to the question\n",
        "a1, b1 = 1.0, 5.0\n",
        "xx = np.linspace(a,b,101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZtSXY-o9sM",
        "colab_type": "text"
      },
      "source": [
        "**(c) The \"true\" error is always less than the estimated error - why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBVk-6Pqo9sN",
        "colab_type": "text"
      },
      "source": [
        "### Algorithm #2: Fixed-point iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lajp5yFXo9sO",
        "colab_type": "text"
      },
      "source": [
        "Unlike recursive bisection, there are multiple possible Fixed Point Iterations for a given function. Recall that for the original equation  $f(x) = 0$, we need to define a $\\varphi(x)$ such that the equation can be rearranged to:\n",
        "$$\n",
        "x = \\varphi(x)\n",
        "$$\n",
        "\n",
        "Then an initial guess $x_0$ generates a sequence of estimates for the fixed point by setting:\n",
        "$$\n",
        " x_{k+1} = \\varphi(x_k), \\hspace{1cm} k= 1,2,...\n",
        "$$\n",
        "We will consider the function $f(x) = \\cos x - 2x$\n",
        "\n",
        "First choose a $\\varphi(x)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d74EIYmPo9sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def phi(x): return 0.5*np.cos(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gswb0gkro9sS",
        "colab_type": "text"
      },
      "source": [
        "The implementation is extremely concise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdZEBGAzo9sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fixed_point(phi, x0, n_max):\n",
        "    plot_fpi = [x0]                  ### Store intermediate results for plotting\n",
        "    xi = x0\n",
        "    for i in range(n_max):\n",
        "        xi = phi(xi)\n",
        "        plot_fpi.append(xi)          ### Store intermediate results for plotting\n",
        "    return xi, np.array(plot_fpi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTWqZ_axo9sW",
        "colab_type": "text"
      },
      "source": [
        "Again, visualization helps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhtGX9byo9sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FPI_progress_plot(phi, a, b, plot_fpi):\n",
        "    xx = np.linspace(a,b,101)        ### change the values of a and b to zoom in on the \n",
        "                                     ### plot to get a clearer view of the behaviour of \n",
        "                                     ### the method  \n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(xx, phi(xx), '-k', label=r'$\\varphi$')       ### plot function\n",
        "    plt.plot(xx, xx, '-r', label=r'$x=y$')                ### plot y=x\n",
        "    for i in range(len(plot_fpi)-1):                      ### plot construction lines \n",
        "        plt.plot([plot_fpi[i],plot_fpi[i]], [plot_fpi[i], phi(plot_fpi[i])], '-ob')\n",
        "        plt.plot([plot_fpi[i],plot_fpi[i+1]], [phi(plot_fpi[i]), phi(plot_fpi[i])], '-ob')\n",
        "    plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNi0qCpjo9sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0 = 1.0\n",
        "a, b = -2, 2\n",
        "xreference, plot_fpi = fixed_point(phi, x0, 50)\n",
        "FPI_progress_plot(phi,a,b,plot_fpi)\n",
        "xreference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dPIBVJHo9sd",
        "colab_type": "text"
      },
      "source": [
        "And the convergence plot (we don't have an good error estimate $\\bar e_k$, here unlike for RB):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqHIVu18o9se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Ignore the last point in the convegence histories, that's\n",
        "### the reference!\n",
        "def FPI_convergence(plot_fpi, xreference, f_FPI):\n",
        "    n = len(plot_fpi)-1\n",
        "    nn = range(n)\n",
        "                                           ### Compute the errors\n",
        "    plot_fpi_error = np.abs(plot_fpi[:-1] - xreference)\n",
        "    plot_fpi_residual = np.abs(f_FPI(plot_fpi[:-1]))\n",
        "                                           ### Plot the errors against n\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(nn, np.log10(plot_fpi_error), '-o', label=r'$\\hat e_k$ - Approximate error')\n",
        "    plt.plot(nn, np.log10(plot_fpi_residual), '-o', label=r'$\\epsilon_k$ - Residual error')\n",
        "    plt.xlabel(r'$k$', fontsize=20)\n",
        "    plt.ylabel(r'$\\mathrm{log}_{10}(e)$', fontsize=20)\n",
        "    plt.legend()\n",
        "                                           ### Plot the convergence rate against n\n",
        "    plt.subplot(122)\n",
        "    plt.plot(nn[:-1], plot_fpi_residual[1:]/plot_fpi_residual[:-1], '-og')\n",
        "    plt.xlabel(r'$k$', fontsize=20)\n",
        "    plt.ylabel(r'$e_{k+1}/e_k$', fontsize=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMxuxbRxo9si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FPI_convergence(plot_fpi, xreference, f_cos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nOesEaho9sm",
        "colab_type": "text"
      },
      "source": [
        "We've also plotted the <i>convergence rate</i> - i.e. the factor by which the error is reduced from one iteration to the next.  This plot has a plateau corresponding to the straight-line reduction of error in the left-hand plot.  [Convince yourself of this relationship; how would the left-hand plot change if the plateau moved up/down?]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LthdDitdo9sn",
        "colab_type": "text"
      },
      "source": [
        "#### Stability of fixed-point iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc2mWaYxo9so",
        "colab_type": "text"
      },
      "source": [
        "From the notes we know that for FPI to be stable and avoid divergence,\n",
        "$$\n",
        "\\left|\\frac{d\\varphi(x)}{dx}\\right| < 1 \\quad \\mbox{for all}\\quad x\\in [x_0, \\tilde x].\n",
        "$$\n",
        "We can verify by that the choice of $\\varphi(x)$ used above satisfies the condition near the exact root $\\tilde x$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kctEz6o0o9sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dphidx(x): return 0.5*(-np.sin(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyj_Zq62o9ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = np.linspace(a,b,101)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.subplot(211)\n",
        "plt.plot(xx, f_cos(xx), '-b', label=r'$f(x)$')       ### plot function\n",
        "plt.plot(xx, xx*0, '-k', label=r'$y=0$')      ### plot y=0\n",
        "plt.legend()\n",
        "plt.subplot(212)\n",
        "plt.plot(xx, dphidx(xx), '-r', label=r'$|d\\varphi(x)/dx|$')  ### plot function\n",
        "plt.plot(xx, xx*0+1, '-k', label=r'$y=1$')      ### plot y=0\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56RkCTh8o9sv",
        "colab_type": "text"
      },
      "source": [
        "<b>Exercise 2:</b>\n",
        "\n",
        "**(a) Are these results consistent with the theory?  In particular, how is the rate of convergence related to $\\varphi'(\\tilde x)$?  Compute $\\varphi'(\\tilde x)$ at the root and compare it with the plateau in the convergence-rate plot.  Do they agree?  Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk-oxawEo9sw",
        "colab_type": "text"
      },
      "source": [
        "**(b) Consider the function $x=\\tan x$. Select an appropriate $\\varphi(x)$ and try and plot the values in the interval $(0,4.6)$. Are there any roots of $f(x)$ (outside the interval), to which the FPI will not converge?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W87kbUoko9sw",
        "colab_type": "text"
      },
      "source": [
        "**(c) Consider a different choice of $\\varphi(x)$.  How many possible choices are there?  Can you find a $\\varphi(x)$ that converges to the roots which the other $\\varphi$ didn't converge to?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NowPQgoao9sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO\n",
        "a,b = 0.0,4.6\n",
        "xx = np.linspace(a,b,101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs10N3zQo9s0",
        "colab_type": "text"
      },
      "source": [
        "### Algorithm #3: Newton's method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BofmMqo9s1",
        "colab_type": "text"
      },
      "source": [
        "Newton's method is the most popular solution method, because it works for functions of arbitrarily many variables (unlike recursive bisection), and does not require careful choice of $\\varphi$ (unlike FPIs).  Newton's method can be expressed as:\n",
        "$$\n",
        "x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)},\n",
        "$$\n",
        "so requires the derivative of $f$ as an argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyT9ZzXHo9s2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton(f, dfdx, x0, n_max):\n",
        "    plot_newton = [x0]                  ### Store intermediate results for plotting\n",
        "    xi = x0\n",
        "    for i in range(n_max):\n",
        "        xi = xi - f(xi)/dfdx(xi)\n",
        "        plot_newton.append(xi)          ### Store intermediate results for plotting\n",
        "    return xi, np.array(plot_newton)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeFZQFa3o9s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton_progress_plot(a,b,fn):\n",
        "    xx = np.linspace(a,b,101)\n",
        "\n",
        "    plt.figure(figsize=(14,10))\n",
        "    plt.plot(xx, fn(xx), '-b', label=r'$f(x)$')       ### plot function\n",
        "\n",
        "    plt.plot(xx, xx*0, '-k', label=r'$y=0$')         ### plot y=0\n",
        "    for i in range(len(plot_newton)-1):                    ### plot construction lines \n",
        "        plt.plot([plot_newton[i],plot_newton[i]], [0,fn(plot_newton[i])], '-or')\n",
        "        plt.plot([plot_newton[i],plot_newton[i+1]], [fn(plot_newton[i]),0], '-or')\n",
        "    plt.legend()\n",
        "    plt.ylim(-2,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E06HUOvFo9s7",
        "colab_type": "text"
      },
      "source": [
        "For the Newton's method, the selection of the initial value is important for convergence. For the polynomial function, notice that $f(-2) = 5$ and $f(-1) = 1$. Hence we can deduce that the root is between -2 and 1. So let us start with an initial value of $x_0 = -1$  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p06Cm57po9s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0 = -1.\n",
        "a,b = -2,2\n",
        "xref_newton, plot_newton = newton(f_poly, dfdx_poly, x0, 20)\n",
        "newton_progress_plot(a,b,f_poly)\n",
        "xref_newton\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgGD7jzo9s_",
        "colab_type": "text"
      },
      "source": [
        "Plotting the behaviour of the method has given us some insights (hopefully).A good indication of the general behaviour of the Newton method can be seen from the following link\n",
        "$\\href{https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif}{[2]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eugUm1BSo9tA",
        "colab_type": "text"
      },
      "source": [
        "Try the newton method for the f_cos function and  see if the method converges to the root (Plotting the function in Section 1 would be a good idea to see where the root lies so that we can pick an initial value accordingly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTZ5tl20o9tB",
        "colab_type": "text"
      },
      "source": [
        "The convergence plot for Newton is qualitatively different from that of Recursive Bisection and Fixed Point Iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqq-6vG5o9tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton_convergence(plot_newton,xref,f):\n",
        "                                             ### Ignore the last point in the convegence \n",
        "                                             ### histories, that's the reference!\n",
        "    n = len(plot_newton)-1\n",
        "    nn = range(n)\n",
        "                                             ### Compute the errors\n",
        "    plot_newton_error = np.abs(plot_newton[:-1] - xref)\n",
        "    plot_newton_residual = np.abs(f(plot_newton[:-1]))\n",
        "                                             ### Plot the errors against n\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(nn, np.log10(plot_newton_error), '-o', label='True error in $x$')\n",
        "    plt.plot(nn, np.log10(plot_newton_residual), '-o', label='Residual error')\n",
        "    plt.xlabel(r'$n$', fontsize=20)\n",
        "    plt.ylabel(r'$\\mathrm{log}_{10}(e)$', fontsize=20)\n",
        "    plt.legend()\n",
        "                                             ### Plot the convergence rate against n\n",
        "    plt.subplot(122)\n",
        "    plt.plot(nn[:-1], plot_newton_residual[1:]/plot_newton_residual[:-1], '-og')\n",
        "    plt.xlabel(r'$n$', fontsize=20)\n",
        "    plt.ylabel(r'$e_{n+1}/e_n$', fontsize=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THGPXDlvo9tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newton_convergence(plot_newton,xref_newton,f_poly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaaixUExo9tK",
        "colab_type": "text"
      },
      "source": [
        "Newton's method is a fixed-point iteration with $\\varphi(x)$ choosen as:\n",
        "$$\n",
        "\\varphi(x) = x - \\frac{f(x)}{f'(x)}\n",
        "$$\n",
        "and we saw in the notes that\n",
        "$$\n",
        "\\varphi'(x) = \\frac{f(x)f''(x)}{(f'(x))^2},\n",
        "$$\n",
        "Plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESgPa4-o9tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dphi_newtondx(x,f,fp,fpp): return f(x)*fpp(x) / (fp(x)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWG-vrBo9tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = np.linspace(a,b,101)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.subplot(211)\n",
        "plt.plot(xx, f_poly(xx), '-b', label=r'$f(x)$')    ### plot function\n",
        "plt.plot(xx, xx*0, '-k', label=r'$y=0$')      ### plot y=0\n",
        "plt.legend()\n",
        "plt.subplot(212)\n",
        "plt.plot(xx, np.abs(dphi_newtondx(xx,f_poly,dfdx_poly,d2fdx2_poly)), '-r', \n",
        "             label=r'$|d\\varphi(x)/dx|$')  ### plot dphi'\n",
        "plt.plot(xx, xx*0, '-k', label=r'$y=0$')      ### plot y=0\n",
        "plt.plot(xx, xx*0+1, '-k', label=r'$y=1$')      ### plot y=1\n",
        "plt.ylim(-1,3)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vd2E5D3o9tR",
        "colab_type": "text"
      },
      "source": [
        "And we see that at the root $\\varphi'(\\tilde x) =0$ , explaining the amazing performance of Newton's method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VkQHfMoo9tS",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 3:**\n",
        "\n",
        "**(a) We examine the importance of the initial value $x_0$.**\n",
        "\n",
        "  0. **The function $f(x) =xe^{-x^2}$ has a zero at $x=0$. Use the Newton's method to try and find this root using an initial value $x_0 =1$.**  Hint: Plotting the graph of the function might explain this peculiar behaviour\n",
        "\n",
        "  0. **What happens and why when the initial value $x_0 = 0.5$ is chosen?**\n",
        "\n",
        "  0. **Now try to find an initial value that will lead to the solution.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27gvOZ9o9tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enwPZOY7o9tV",
        "colab_type": "text"
      },
      "source": [
        "**(b) Convergence.  We shall consider the same function $f(x) =\\tan x - x$ in the interval $(-1.0,5).$ and try and find all roots in the interval (zero and positive) using the Newton method. Since we have already checked the sensitivity of the initial value in the previous question, use initial values of $x_0 = -1$ and $x_0 = 4.$ for finding the roots. Is there any difference between the rate of convergence for the two roots? Is this result consistent with the theory learnt in the lectures?** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qPqOZsCo9tW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO\n",
        "# Again use the already defined functions for the newton method calculation,plotting and convergence\n",
        "a, b = -.5, 5.\n",
        "xx = np.linspace(a,b,101)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFBsKcHo9ta",
        "colab_type": "text"
      },
      "source": [
        "### Finally: Multi-dimensional problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v57_GJho9tb",
        "colab_type": "text"
      },
      "source": [
        "The real power (and necessity) for root-finding methods occurs in the multidimensional case:\n",
        "$$\n",
        "f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\n",
        "$$\n",
        "i.e. $f$ has $n$ inputs and $n$ outputs.  Then the equation\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{0}\n",
        "$$\n",
        "represents a system of $n$ non-linear equations in $n$ unknowns.  In the <i>linear</i> case (where $f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$), we know exactly when these problems have unique solutions, and we have methods for finding them (Linear Algebra).  In the non-linear case, unless we're lucky, we need to use numerical methods.  \n",
        "\n",
        "The derivation of Newton's method becomes: the Taylor series of $f$ at $x_k$ iswritten:\n",
        "$$\n",
        "f(\\mathbf{x}) = f(\\mathbf{x}_k) + J(\\mathbf{x}_k) (\\mathbf{x}-\\mathbf{x}_k) + \\mathcal{O}(\\mathbf{x}-\\mathbf{x}_k)^2\n",
        "$$\n",
        "where \n",
        "$$\n",
        "J(\\mathbf{x}) := \\left( \\begin{array}{ccc}\n",
        "             \\frac{\\partial f_1(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(x)}{\\partial x_n} \\\\\n",
        "             \\vdots & \\ddots & \\vdots \\\\\n",
        "             \\frac{\\partial f_n(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_n(x)}{\\partial x_n} \\end{array} \\right)\n",
        "$$ \n",
        "is the $(n\\times n)$ Jacobian matrix.  Neglecting higher-order terms, and setting the intercept to $0$ at the $k+1$th iteration gives:\n",
        "$$\n",
        "0 = f(\\mathbf{x}_k) + J(\\mathbf{x}_k) (\\mathbf{x}_{k+1}-\\mathbf{x}_k),\n",
        "$$\n",
        "leading to an expression for the Newton iteration\n",
        "$$ \n",
        "\\mathbf{x}_{k+1} = \\mathbf{x}_k - J(\\mathbf{x}_k)^{-1} f(\\mathbf{x}_k).\n",
        "$$\n",
        "Each iteration requires:\n",
        "\n",
        "  0. Evaluation of $f(\\mathbf{x}_k)$.\n",
        "  0. Evaluation of $J(\\mathbf{x}_k)$.\n",
        "  0. Solution of a linear system of algebraic equations $J(\\mathbf{x}_k) \\Delta \\mathbf{x}_k = -f(\\mathbf{x}_k)$ for the update.\n",
        "  0. Application of the update $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\Delta \\mathbf{x}_k$.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dr_r6NDo9tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton_mdimensional(f, dfdx, x0, n_max):\n",
        "    plot_newtonm = [copy(x0)]\n",
        "    xi = x0\n",
        "    for i in range(n_max):\n",
        "        deltaxi = np.linalg.solve(dfdx(xi), -f(xi))\n",
        "        xi += deltaxi\n",
        "        plot_newtonm.append(copy(xi))\n",
        "    return xi, np.array(plot_newtonm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7M4RSnko9te",
        "colab_type": "text"
      },
      "source": [
        "#### A sample 2-dimensional problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAHNb4xko9tf",
        "colab_type": "text"
      },
      "source": [
        "Find $x$, $y$ such that:\n",
        "$$\n",
        "x^2+y^2=1, \\quad xy=\\frac{1}{4}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah4D6ZsTo9tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(xv): \n",
        "    x, y = xv[0], xv[1]\n",
        "    return np.array([1-x**2-y**2, .25 - x*y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpk0UyTwo9ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dfdx(xv):\n",
        "    x, y = xv[0], xv[1]\n",
        "    return np.array([[-2*x,-2*y],\n",
        "                     [  -y,  -x]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iavinvpJo9tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0 = np.array([-2.0,1.0])\n",
        "xreference, plot_newtonm = newton_mdimensional(f, dfdx, x0, 10)\n",
        "xreference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxYumuc7o9tr",
        "colab_type": "text"
      },
      "source": [
        "It's a good idea to check the residual, to see if this is really a solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDyrmCno9tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f(xreference)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7SkBkFIo9ty",
        "colab_type": "text"
      },
      "source": [
        "We can also look at the path the solution took in the $(x,y)$ plane:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwJf4hpuo9tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = np.linspace(-2.,2.,101)\n",
        "plt.figure(figsize=(8,8))\n",
        "### Plot lines on which individual equations are satisfied\n",
        "plt.plot(xx, np.sqrt(1-xx**2), '-k', label='Eqn 1')\n",
        "plt.plot(xx, -np.sqrt(1-xx**2), '-k')\n",
        "plt.plot(xx, 1./(4*xx), '-r', label='Eqn 2')\n",
        "plt.xlim(-2,2); plt.ylim(-2,2)\n",
        "### Plot path of Newton\n",
        "n = len(plot_newtonm)\n",
        "plt.plot(plot_newtonm[:,0], plot_newtonm[:,1], '-ob')\n",
        "plt.legend()\n",
        "plot_newtonm[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9W4YUQio9t2",
        "colab_type": "text"
      },
      "source": [
        "How many roots of this equation exist?  From which $x_0$ does Newton converge to which root?  Are there values of $x_0$ for which it doesn't converge at all?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFcSu0Zo9t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}